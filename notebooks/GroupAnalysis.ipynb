{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Analysis\n",
    "\n",
    "Desired output figure:\n",
    "- time x classification accuracy\n",
    "- Within, Pre & Post alignment\n",
    "\n",
    "\n",
    "Downsamples to 100 Hz, time-point by time-point classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mne import read_epochs\n",
    "from mne import set_log_level\n",
    "\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from hypertools.tools.align import align\n",
    "\n",
    "# Classification stuff\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_log_level('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Note: Decode both to the stimulus, and to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data size\n",
    "#   Note: these are set for arbitrary test data - need updating for 'real' data\n",
    "n_epochs = 40\n",
    "n_chs = 128\n",
    "#n_times = 1001\n",
    "n_per_cond = int(n_epochs / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxabs(dat, dim):\n",
    "    return np.max(np.abs(dat), dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Set the collection of ways to average across features\n",
    "AVGS = {\n",
    "    'maxabs' : maxabs,\n",
    "    'max' : np.max, \n",
    "    'min' : np.min, \n",
    "    'mean' : np.mean, \n",
    "    'median' : np.median\n",
    "}\n",
    "\n",
    "# Classification Settings\n",
    "K_FOLD = 3\n",
    "AVG_TO_USE = 'mean'\n",
    "\n",
    "# Initialize SVM classification object\n",
    "#CLF = svm.SVC(kernel='linear')\n",
    "CLF = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dat):\n",
    "    \"\"\"Organize data from MNE object, to data matrices and labels to be used for classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : mne.Epochs object\n",
    "        A subject's worth of epoched data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels : 1d array\n",
    "        Labels for each trial type. \n",
    "    data : 3d array\n",
    "        Epoched data matrix. \n",
    "    \"\"\"\n",
    "\n",
    "    # Check event codes there are, and unpack\n",
    "    ev_counts = Counter(dat.events[:, 2])\n",
    "    evc_a, evc_b = dat.event_id.keys()    \n",
    "    n_evc_a, n_evc_b = ev_counts.values()\n",
    "        \n",
    "    # Generate labels\n",
    "    lab_a = np.ones(shape=[n_per_cond]) * -1\n",
    "    lab_b = np.ones(shape=[n_per_cond])\n",
    "\n",
    "    # Extract trial data\n",
    "    eps_a = dat[evc_a]._data[0:n_per_cond, 0:128, :]\n",
    "    eps_b = dat[evc_b]._data[0:n_per_cond, 0:128, :]\n",
    "\n",
    "    # Check all our shapes and sizes are correct\n",
    "    assert len(lab_a) == np.shape(eps_a)[0]\n",
    "    assert len(lab_b) == np.shape(eps_b)[0]\n",
    "    assert len(lab_a) == len(lab_b)\n",
    "    assert np.shape(eps_a)[0] == np.shape(eps_b)[0]\n",
    "    \n",
    "    # Collect all labels and trial data together\n",
    "    data = np.concatenate([eps_a, eps_b], 0)\n",
    "    labels = np.hstack([lab_a, lab_b])\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def make_2d(dat, z_score=True):\n",
    "    \"\"\"Reorganize a 3D matrix into a continuous 2D matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 3d\n",
    "        Epoched data matrix, as [n_epochs, n_channels, n_times]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    2d array\n",
    "        Continuous data matrix of epochs concatendat in time, as [n_channels, n_times_tot]\n",
    "            Note: where n_times_tot = n_times * n_epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    dat = np.concatenate(dat, 1)\n",
    "    \n",
    "    if z_score:\n",
    "        dat = zscore(dat, 0)\n",
    "    \n",
    "    return dat\n",
    "\n",
    "\n",
    "def make_3d(dat):\n",
    "    \"\"\"Reorganize a 2D matrix into the 3D trial structure matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 2d array\n",
    "        Continuous data matrix of epochs concatendat in time, as [n_channels, n_times_tot]\n",
    "            Note: where n_times_tot = n_times * n_epochs\n",
    "        \n",
    "    Results\n",
    "    -------\n",
    "    3d array\n",
    "        Epoched data matrix, as [n_epochs, n_channels, n_times]\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.stack(np.split(dat, n_epochs, 1))\n",
    "    \n",
    "    \n",
    "def within_subj_classification(all_data, all_labels):\n",
    "    \"\"\"Run within subject classification within each subject for a list of subjects data.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : list of 3d array\n",
    "        xx\n",
    "    labels : list of 1d array\n",
    "        xx\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scores : 1d array\n",
    "        xx\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run cross-validated classification within each subject\n",
    "    within_scores = np.zeros(shape=[len(all_data), K_FOLD])\n",
    "    for s_ind, subj_data, subj_labels in zip(range(n_subjs), all_data, all_labels):\n",
    "        within_scores[s_ind, :] = cross_val_score(CLF, feature_dat(subj_data), subj_labels, cv=K_FOLD)\n",
    "    \n",
    "    return within_scores\n",
    "\n",
    "\n",
    "def btwn_subj_classication(all_data, all_labels):\n",
    "    \"\"\"Run classification between subjects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_data : list of 3d array\n",
    "        Data for each subject.\n",
    "    all_labels : list of 1d array\n",
    "        Labels for each subject.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scores : list of float\n",
    "        The classifications scores for each held out subject, as predicted from the group. \n",
    "    \"\"\"\n",
    "\n",
    "    scores = [None] * len(all_data)\n",
    "    \n",
    "    for ind, subj_data, subj_labels in zip(range(len(all_data)), all_data, all_labels):\n",
    "\n",
    "        # Take a copy of the group data, and drop held out subject\n",
    "        temp_data = deepcopy(all_data)\n",
    "        temp_labels = deepcopy(all_labels)\n",
    "        del temp_data[ind]\n",
    "        del temp_labels[ind]\n",
    "\n",
    "        # Collapse group for training the model\n",
    "        group_data = feature_dat(np.concatenate(temp_data, 0))\n",
    "        group_labels = np.concatenate(temp_labels, 0)\n",
    "\n",
    "        # Train on group & classify left out subject\n",
    "        CLF.fit(group_data, group_labels)\n",
    "        scores[ind] = CLF.score(feature_dat(subj_data), subj_labels)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def feature_dat(dat, avg_type=AVG_TO_USE):\n",
    "    \"\"\"Convert epochs \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 3d array\n",
    "        xx\n",
    "    avg_type : {'max', 'min', 'mean', 'median', 'maxabs'}\n",
    "        xx\n",
    "    z_scores : bool\n",
    "        xx\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : XX\n",
    "        xx\n",
    "    \"\"\"\n",
    "\n",
    "    avg = AVGS[avg_type]\n",
    "\n",
    "    # Note: can add something here to select channels / time points\n",
    "    out = avg(dat[:, :, :], 2)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def print_avg(label, score):\n",
    "    print(label + ': {:1.2f}%'.format(score *100))\n",
    "\n",
    "\n",
    "def print_avgs(label, scores):\n",
    "    print(label + ':')\n",
    "    for ind, score in enumerate(scores):\n",
    "        print('\\t{:1.0f} \\t {:1.2f}'.format(ind, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data location for processed files\n",
    "dat_path = '/Users/tom/Desktop/HyperEEG_Project/Data/proc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available files\n",
    "#  Note: this currently excludes first subject, because they are weird.\n",
    "dat_files = [file for file in os.listdir(dat_path)[1:] if 'epo.fif' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "all_subjs = [read_epochs(os.path.join(dat_path, f_name),\n",
    "                         preload=True, verbose=None) for f_name in dat_files]\n",
    "\n",
    "# Downsample data\n",
    "all_subjs = [dat.resample(100) for dat in all_subjs]\n",
    "\n",
    "# Check how many subjects there are\n",
    "n_subjs = len(all_subjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTS\n",
    "\n",
    "# # Load single subject data - and fix channel subset\n",
    "# dat = read_epochs(os.path.join(dat_path, dat_files[2]), preload=True, verbose=False)\n",
    "# dat._data = dat._data[:, 0:128, :]\n",
    "\n",
    "# # Make test list of multi-subj data\n",
    "# n_group = 3\n",
    "# all_subjs = [dat] * n_group\n",
    "# n_subjs = len(all_subjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize subject data into data and label matrices\n",
    "all_data, all_labels = [], []\n",
    "\n",
    "for subj in all_subjs:\n",
    "    \n",
    "    # Enforce a minimum number of trials - skip subj if not met\n",
    "    if len(subj) < n_epochs:\n",
    "        continue\n",
    "        \n",
    "    t_data, t_labels = extract_data(subj)\n",
    "    all_data.append(t_data)\n",
    "    all_labels.append(t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Subject Classification (un-aligned)\n",
    "\n",
    "Notes:\n",
    "- Update to predict across windows of the trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run within subject classification\n",
    "within_scores = within_subj_classification(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results - within and across subjects\n",
    "within_subj_avgs = np.mean(within_scores, 1)\n",
    "within_glob_avg = np.mean(within_subj_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Within-Subj Prediction: 50.25%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('CV Within-Subj Prediction', within_glob_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Subj Within Predictions:\n",
      "\t0 \t 0.50\n",
      "\t1 \t 0.50\n",
      "\t2 \t 0.40\n",
      "\t3 \t 0.50\n",
      "\t4 \t 0.50\n",
      "\t5 \t 0.50\n",
      "\t6 \t 0.50\n",
      "\t7 \t 0.50\n",
      "\t8 \t 0.63\n",
      "\t9 \t 0.50\n",
      "\t10 \t 0.50\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Per Subj Within Predictions', within_subj_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between Subject Classification (un-aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction between subjects - on unaligned data\n",
    "btwn_scores = btwn_subj_classication(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results\n",
    "avg_btwn_scores = np.mean(btwn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn-Subj Prediction: 52.50%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('Btwn-Subj Prediction', avg_btwn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn Subject Classification:\n",
      "\t0 \t 0.53\n",
      "\t1 \t 0.50\n",
      "\t2 \t 0.47\n",
      "\t3 \t 0.50\n",
      "\t4 \t 0.50\n",
      "\t5 \t 0.62\n",
      "\t6 \t 0.50\n",
      "\t7 \t 0.50\n",
      "\t8 \t 0.62\n",
      "\t9 \t 0.53\n",
      "\t10 \t 0.50\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Btwn Subject Classification', btwn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment (Hypertools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data organization - extract matrices, and flatten to continuous data\n",
    "all_data = [make_2d(dat) for dat in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do alignment\n",
    "#  Note: this also switches orientation (takes the transpose) to match hypertools\n",
    "aligned_data = align([dat.T for dat in all_data]) # Note: align assumes [n_samples x n_channels]\n",
    "aligned_data = [dat.T for dat in aligned_data]\n",
    "aligned_data = [make_3d(dat) for dat in aligned_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between Subject Classification (aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Aligned: 51.59%\n",
      "\n",
      "Per subj Within-Aligned:\n",
      "\t0 \t 0.52\n",
      "\t1 \t 0.53\n",
      "\t2 \t 0.40\n",
      "\t3 \t 0.53\n",
      "\t4 \t 0.60\n",
      "\t5 \t 0.49\n",
      "\t6 \t 0.42\n",
      "\t7 \t 0.48\n",
      "\t8 \t 0.55\n",
      "\t9 \t 0.57\n",
      "\t10 \t 0.58\n"
     ]
    }
   ],
   "source": [
    "# Check within subject prediction of aligned data\n",
    "within_al1_scores = within_subj_classification(aligned_data, all_labels)\n",
    "print_avg('Within Aligned', np.mean(within_al1_scores))\n",
    "print_avgs('\\nPer subj Within-Aligned', np.mean(within_al1_scores, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction between subjects - on aligned data\n",
    "btwn_al_scores = btwn_subj_classication(aligned_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results\n",
    "avg_btwn_al_scores = np.mean(btwn_al_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn-Subj Prediction: 80.91%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('Btwn-Subj Prediction', avg_btwn_al_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn Subject Classification:\n",
      "\t0 \t 0.88\n",
      "\t1 \t 0.78\n",
      "\t2 \t 0.65\n",
      "\t3 \t 0.93\n",
      "\t4 \t 0.85\n",
      "\t5 \t 0.85\n",
      "\t6 \t 0.90\n",
      "\t7 \t 0.82\n",
      "\t8 \t 0.70\n",
      "\t9 \t 0.65\n",
      "\t10 \t 0.90\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Btwn Subject Classification', btwn_al_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKS\n",
    "Compare hyperaligned to unaligned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: these checks are for test cases which use a group of the same data copied over\n",
    "#print(np.all(all_data[0] == all_data[1]))\n",
    "#print(np.all(aligned_data[0] == aligned_data[1]))\n",
    "#print(np.all(aligned_data[0] == all_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check random rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random rotation matrix\n",
    "#rot = np.random.random(size=n_chs*n_chs).reshape([n_chs, n_chs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation by random matrix\n",
    "#twod_dat = deepcopy(all_data)\n",
    "#twod_dat = [np.dot(rot, dat) for dat in twod_dat]\n",
    "#twod_dat_3d = [make_3d(dat) for dat in twod_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check within subject prediction of random data\n",
    "#within_rand_scores = within_subj_classification(twod_dat_3d, all_labels)\n",
    "#print_avg('Within Rand', np.mean(within_rand_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between subject classification\n",
    "#rand_btwn_scores = btwn_subj_classication(twod_dat_3d, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outcome - average across all subjects\n",
    "#avg_rand_btwn = np.mean(rand_btwn_scores)\n",
    "#print_avg('Random Btwn-Subj Prediction', avg_rand_btwn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyMVPA\n",
    "\n",
    "Apply hyperalignment implementation from the PyMVPA package.\n",
    "\n",
    "Note: this requires being in a Py2 environment with PyMVPA available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.datasets.base import Dataset\n",
    "from mvpa2.algorithms.hyperalignment import Hyperalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize data into PyMVPA datasets objects\n",
    "datasets = [Dataset(dat.T) for dat in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperalignment, and get the transformation matrices\n",
    "hyper_aligner = Hyperalignment(level2_niter=0, zscore_all=False, zscore_common=False)\n",
    "hyper_aligner.train(datasets)\n",
    "mappers = hyper_aligner(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations to each dataset, and re-organize data\n",
    "#   This applies the projection to the 2D data, transpose, and split back into epochs\n",
    "aligned_datasets = []\n",
    "for dataset, mapper in zip(datasets, mappers):\n",
    "    aligned_datasets.append(make_3d(mapper.forward(dataset).samples.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Within Al2: 52.45%\n"
     ]
    }
   ],
   "source": [
    "# Check within subject prediction of aligned data\n",
    "within_al2_scores = within_subj_classification(aligned_datasets, all_labels)\n",
    "print_avg('Within Al2', np.mean(within_al2_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between subject classification after PyMVPA hyperalignment\n",
    "btwn_al2_scores = btwn_subj_classication(aligned_datasets, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned-2 Btwn Scores: 85.00%\n"
     ]
    }
   ],
   "source": [
    "# Check average performance\n",
    "avg_btwn_al2 = np.mean(btwn_al2_scores)\n",
    "print_avg('Aligned-2 Btwn Scores', avg_btwn_al2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between Subj Aligned Data:\n",
      "\t0 \t 0.90\n",
      "\t1 \t 0.85\n",
      "\t2 \t 0.80\n",
      "\t3 \t 0.90\n",
      "\t4 \t 0.82\n",
      "\t5 \t 0.85\n",
      "\t6 \t 0.97\n",
      "\t7 \t 0.93\n",
      "\t8 \t 0.68\n",
      "\t9 \t 0.68\n",
      "\t10 \t 0.97\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "print_avgs('Between Subj Aligned Data', btwn_al2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the rotation matrices are the same\n",
    "#np.all(mappers[0].proj == mappers[1].proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare between the two hyperalignment implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the magnitude of differences between aligned data\n",
    "#diff = aligned_datasets[0] - aligned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Avg Magnitude Diff', np.mean(np.abs(diff)))\n",
    "#print('Avg Magnitude Data', np.mean(np.abs(aligned_datasets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of overlapping points\n",
    "#from __future__ import division\n",
    "#np.sum(np.isclose(aligned_datasets[0], aligned_data[0])) / aligned_data[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victory Party.\n",
    "\n",
    "Soon..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
