{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Analysis\n",
    "\n",
    "Desired output figure:\n",
    "- time x classification accuracy\n",
    "- Within, Pre & Post alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mne import read_epochs\n",
    "\n",
    "from hypertools.tools.align import align\n",
    "\n",
    "# Classification stuff\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Note: Decode both to the stimulus, and to the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Settings\n",
    "k_fold = 3\n",
    "\n",
    "# Initialize SVM classification object\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Set the expected number of events per condition\n",
    "expected_ev_counts = [25, 25]\n",
    "\n",
    "# Set data size\n",
    "n_epochs = 50\n",
    "n_chs = 128\n",
    "n_times = 1001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "\n",
    "# Set the collection of ways to average across features\n",
    "AVGS = {\n",
    "    'max' : np.max, \n",
    "    'min' : np.min, \n",
    "    'mean' : np.mean, \n",
    "    'median' : np.median\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API notes:\n",
    "# - work on subject level objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(dat):\n",
    "    \"\"\"Organize data from MNE object, to data matrices and labels to be used for classification. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : mne.Epochs object\n",
    "        A subject's worth of epoched data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    labels : 1d array\n",
    "        Labels for each trial type. \n",
    "    data : 3d array\n",
    "        Epoched data matrix. \n",
    "    \"\"\"\n",
    "\n",
    "    # Check event codes there are, and unpack\n",
    "    ev_counts = Counter(dat.events[:, 2])\n",
    "    evc_a, evc_b = [str(el) for el in ev_counts.keys()]\n",
    "    n_evc_a, n_evc_b = ev_counts.values()\n",
    "\n",
    "    # Check the number of events is as expected\n",
    "    if [n_evc_a, n_evc_b] != expected_ev_counts:\n",
    "        raise ValueError('Number of events does not match what was expected.')\n",
    "        \n",
    "    # Generate labels\n",
    "    labels = np.hstack([np.zeros(shape=[n_evc_a]), np.ones(shape=[n_evc_b])])\n",
    "    \n",
    "    # Organize data\n",
    "    data = np.concatenate([dat[evc_a]._data, dat[evc_b]._data], 0)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "\n",
    "def make_2d(dat):\n",
    "    \"\"\"Reorganize a 3D matrix into a continuous 2D matrix. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 3d\n",
    "        Epoched data matrix, as [n_epochs, n_channels, n_times]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    2d array\n",
    "        Continuous data matrix of epochs concatendat in time, as [n_channels, n_times_tot]\n",
    "            Note: where n_times_tot = n_times * n_epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.concatenate(dat, 1)\n",
    "\n",
    "\n",
    "def make_3d(dat):\n",
    "    \"\"\"Reorganize a 2D matrix into the 3D trial structure matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 2d array\n",
    "        Continuous data matrix of epochs concatendat in time, as [n_channels, n_times_tot]\n",
    "            Note: where n_times_tot = n_times * n_epochs\n",
    "        \n",
    "    Results\n",
    "    -------\n",
    "    3d array\n",
    "        Epoched data matrix, as [n_epochs, n_channels, n_times]\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.stack(np.split(dat, n_epochs, 1))\n",
    "    \n",
    "\n",
    "def btwn_subj_classication(all_data, all_labels):\n",
    "    \"\"\"Run classification between subjects.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_data : list of 3d array\n",
    "        Data for each subject.\n",
    "    all_labels : list of 1d array\n",
    "        Labels for each subject.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    scores : list of float\n",
    "        The classifications scores for each held out subject, as predicted from the group. \n",
    "    \"\"\"\n",
    "\n",
    "    scores = [None] * len(all_data)\n",
    "    \n",
    "    for ind, subj_data, subj_labels in zip(range(len(all_data)), all_data, all_labels):\n",
    "\n",
    "        # Take a copy of the group data, and drop held out subject\n",
    "        temp_data = deepcopy(all_data)\n",
    "        temp_labels = deepcopy(all_labels)\n",
    "        del temp_data[ind]\n",
    "        del temp_labels[ind]\n",
    "\n",
    "        # Collapse group for training the model\n",
    "        group_data = feature_dat(np.concatenate(temp_data, 0))\n",
    "        group_labels = np.concatenate(temp_labels, 0)\n",
    "\n",
    "        # Train on group & classify left out subject\n",
    "        clf = svm.SVC(kernel='linear')\n",
    "        clf.fit(group_data, group_labels)\n",
    "        scores[ind] = clf.score(feature_dat(subj_data), subj_labels)\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def feature_dat(dat, avg_type='max'):\n",
    "    \"\"\"Convert epochs \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dat : 3d array\n",
    "        xx\n",
    "    avg_type : {'max', 'min', 'mean', 'median'}\n",
    "        xx\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : XX\n",
    "        xx\n",
    "    \"\"\"\n",
    "\n",
    "    avg = AVGS[avg_type]\n",
    "\n",
    "    # Note: can add something here to select channels / time points\n",
    "    out = avg(dat[:, :, :], 2)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def print_avg(label, score):\n",
    "    print(label + ': {:1.2f}%'.format(score *100))\n",
    "\n",
    "\n",
    "def print_avgs(label, scores):\n",
    "    print(label + ':')\n",
    "    for ind, score in enumerate(scores):\n",
    "        print('\\t{:1.0f} \\t {:1.2f}'.format(ind, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Organization / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data location for processed files\n",
    "dat_path = '/Users/tom/Desktop/HyperEEG_Project/Data/proc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of available files\n",
    "dat_files = [file for file in os.listdir(dat_path) if '.fif' in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data\n",
    "all_subjs = [read_epochs(os.path.join(dat_path, f_name),\n",
    "                         preload=True, verbose=False) for f_name in dat_files]\n",
    "\n",
    "# Check how many subjects there are\n",
    "n_subjs = len(all_subjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TESTS\n",
    "\n",
    "# Load single subject data - and fix channel subset\n",
    "dat = read_epochs(os.path.join(dat_path, dat_files[0]), preload=True, verbose=False)\n",
    "dat._data = dat._data[:, 0:128, :]\n",
    "\n",
    "# Make test list of multi-subj data\n",
    "n_group = 7\n",
    "all_subjs = [dat] * n_group\n",
    "n_subjs = len(all_subjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Subject Classification (un-aligned)\n",
    "\n",
    "Notes:\n",
    "- Update to predict across windows of the trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize subject data for classification\n",
    "all_data, all_labels = [], []\n",
    "for subj in all_subjs:\n",
    "    t_data, t_labels = extract_data(subj)\n",
    "    all_data.append(t_data)\n",
    "    all_labels.append(t_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validated classification within each subject\n",
    "within_scores = np.zeros(shape=[n_subjs, k_fold])\n",
    "for s_ind, subj_data, subj_labels in zip(range(n_subjs), all_data, all_labels):\n",
    "    within_scores[s_ind, :] = cross_val_score(clf, feature_dat(subj_data), subj_labels, cv=k_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results - within and across subjects\n",
    "within_subj_avgs = np.mean(within_scores, 1)\n",
    "within_glob_avg = np.mean(within_subj_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Within-Subj Prediction: 63.66%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('CV Within-Subj Prediction', within_glob_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per Subj Within Predictions:\n",
      "\t0 \t 0.64\n",
      "\t1 \t 0.64\n",
      "\t2 \t 0.64\n",
      "\t3 \t 0.64\n",
      "\t4 \t 0.64\n",
      "\t5 \t 0.64\n",
      "\t6 \t 0.64\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Per Subj Within Predictions', within_subj_avgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between Subject Classification (un-aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction between subjects - on unaligned data\n",
    "btwn_scores = btwn_subj_classication(all_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results\n",
    "avg_btwn_scores = np.mean(btwn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn-Subj Prediction: 58.00%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('Btwn-Subj Prediction', avg_btwn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn Subject Classification:\n",
      "\t0 \t 0.58\n",
      "\t1 \t 0.58\n",
      "\t2 \t 0.58\n",
      "\t3 \t 0.58\n",
      "\t4 \t 0.58\n",
      "\t5 \t 0.58\n",
      "\t6 \t 0.58\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Btwn Subject Classification', btwn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data organization - extract matrices, and flatten to continuous data\n",
    "all_data = [make_2d(dat) for dat in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do alignment\n",
    "#  Note: this also switches orientation (takes the transpose) to match hypertools\n",
    "aligned_data = align([dat.T for dat in all_data]) # Note: align assumes [n_samples x n_channels]\n",
    "aligned_data = [dat.T for dat in aligned_data]\n",
    "aligned_data = [make_3d(dat) for dat in aligned_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between Subject Classification (aligned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction between subjects - on aligned data\n",
    "btwn_al_scores = btwn_subj_classication(aligned_data, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average results\n",
    "avg_btwn_al_scores = np.mean(btwn_al_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn-Subj Prediction: 58.00%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "print_avg('Btwn-Subj Prediction', avg_btwn_al_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Btwn Subject Classification:\n",
      "\t0 \t 0.58\n",
      "\t1 \t 0.58\n",
      "\t2 \t 0.58\n",
      "\t3 \t 0.58\n",
      "\t4 \t 0.58\n",
      "\t5 \t 0.58\n",
      "\t6 \t 0.58\n"
     ]
    }
   ],
   "source": [
    "# Check performance on each subject\n",
    "print_avgs('Btwn Subject Classification', btwn_al_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHECKS\n",
    "Compare hyperaligned to unaligned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tom/anaconda/envs/mvpa/lib/python2.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(np.all(all_data[0] == all_data[1]))\n",
    "print(np.all(aligned_data[0] == aligned_data[1]))\n",
    "print(np.all(aligned_data[0] == all_data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check random rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random rotation matrix\n",
    "rot = np.random.random(size=n_chs*n_chs).reshape([n_chs, n_chs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation by random matrix\n",
    "twod_dat = deepcopy(all_data)\n",
    "twod_dat = [np.dot(rot, dat) for dat in twod_dat]\n",
    "twod_dat_3d = [make_3d(dat) for dat in twod_dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between subject classification\n",
    "rand_btwn_scores = btwn_subj_classication(twod_dat_3d, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Btwn-Subj Prediction: 58.00%\n"
     ]
    }
   ],
   "source": [
    "# Check outcome - average across all subjects\n",
    "avg_rand_btwn = np.mean(rand_btwn_scores)\n",
    "print_avg('Random Btwn-Subj Prediction', avg_rand_btwn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PyMVPA\n",
    "\n",
    "Apply hyperalignment implementation from the PyMVPA package.\n",
    "\n",
    "Note: this requires being in a Py2 environment with PyMVPA available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mvpa2.datasets.base import Dataset\n",
    "from mvpa2.algorithms.hyperalignment import Hyperalignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-organize data into PyMVPA datasets objects\n",
    "datasets = [Dataset(dat.T) for dat in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperalignment, and get the transformation matrices\n",
    "hyper_aligner = Hyperalignment()\n",
    "hyper_aligner.train(datasets)\n",
    "mappers = hyper_aligner(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the transformations to each dataset, and re-organize data\n",
    "#   This applies the projection to the 2D data, transpose, and split back into epochs\n",
    "aligned_datasets = []\n",
    "for dataset, mapper in zip(datasets, mappers):\n",
    "    aligned_datasets.append(make_3d(mapper.forward(dataset).samples.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between subject classification after PyMVPA hyperalignment\n",
    "btwn_al2_scores = btwn_subj_classication(aligned_datasets, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned-2 Btwn Scores: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Check average performance\n",
    "avg_btwn_al2 = np.mean(btwn_al2_scores)\n",
    "print_avg('Aligned-2 Btwn Scores', avg_btwn_al2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the rotation matrices are the same\n",
    "np.all(mappers[0].proj == mappers[1].proj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare between the two hyperalignment implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the magnitude of differences between aligned data\n",
    "diff = aligned_datasets[0] - aligned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Avg Magnitude Diff', 0.7180778440607879)\n",
      "('Avg Magnitude Data', 0.7180805884556001)\n"
     ]
    }
   ],
   "source": [
    "print('Avg Magnitude Diff', np.mean(np.abs(diff)))\n",
    "print('Avg Magnitude Data', np.mean(np.abs(aligned_datasets[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of overlapping points\n",
    "from __future__ import division\n",
    "np.sum(np.isclose(aligned_datasets[0], aligned_data[0])) / aligned_data[0].size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victory Party.\n",
    "\n",
    "Soon..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
